{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import gc\n",
    "import datetime as dt\n",
    "from datetime import datetime, timedelta \n",
    "from openpyxl import load_workbook\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "from sklearn import preprocessing, metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import dask.dataframe as dd\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "import seaborn as sns\n",
    "import collections as co\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import lightgbm as lgb\n",
    "from ipywidgets import widgets, interactive\n",
    "from typing import Union\n",
    "from tqdm.notebook import tqdm_notebook as tqdm\n",
    "from itertools import cycle\n",
    "from fastprogress import master_bar, progress_bar\n",
    "\n",
    "pd.set_option('max_columns', 50)\n",
    "plt.style.use('bmh')\n",
    "color_pal = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "color_cycle = cycle(plt.rcParams['axes.prop_cycle'].by_key()['color'])\n",
    "ngpu = 1\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "current_path = os.getcwd()\n",
    "use_datetime = 'BaseCase'\n",
    "\n",
    "test_input_path = current_path + '\\\\Data\\\\After_Preprocess\\\\' + use_datetime + '\\\\Test\\\\' ## Refrac만 Testdataset으로 할 때 해당 경로를 바꿔주면 됨\n",
    "\n",
    "def Set_Var(Choose_num_of_first_output_parameter, Choose_num_of_second_output_parameter, use_or_not_ProdBOE, use_or_not_DailyRate, use_or_not_CumProdBOE, \n",
    "use_or_not_OilProd, use_or_not_OilRate, use_or_not_CumOil, use_or_not_GasProd, use_or_not_GasRate, use_or_not_CumGas, \n",
    "use_or_not_WaterProd, use_or_not_CumMonth, use_or_not_ProdDays, use_or_not_CumProdDay, use_or_not_Shutin, use_or_not_Refrac):\n",
    "\n",
    "    num_of_features = use_or_not_ProdBOE + use_or_not_DailyRate + use_or_not_CumProdBOE + use_or_not_OilProd + use_or_not_OilRate + use_or_not_CumOil + use_or_not_GasProd + use_or_not_GasRate + use_or_not_CumGas + use_or_not_WaterProd + use_or_not_CumMonth + use_or_not_ProdDays + use_or_not_CumProdDay + use_or_not_Shutin + use_or_not_Refrac\n",
    "    tmp_1 = [use_or_not_ProdBOE, use_or_not_DailyRate, use_or_not_CumProdBOE, use_or_not_OilProd, use_or_not_OilRate, use_or_not_CumOil, use_or_not_GasProd, use_or_not_GasRate, use_or_not_CumGas, use_or_not_WaterProd, use_or_not_CumMonth, use_or_not_ProdDays, use_or_not_CumProdDay, use_or_not_Shutin, use_or_not_Refrac]\n",
    "    tmp_2 = [\"Prod_BOE\", \"ProdRate_BOE\", \"CumProd_BOE\", 'LiquidsProd_BBL', 'Liq_rate', 'CumLiquids_BBL', 'GasProd_MCF', 'Gas_rate','CumGas_MCF', 'WaterProd_BBL', \"TotalProdMonths\", \"ProducingDays\", 'CumProdDay', \"ShutinMonths\", \"Refrac\"]\n",
    "    used_features = []\n",
    "\n",
    "    used_features = np.append(used_features, tmp_2[Choose_num_of_first_output_parameter - 1])\n",
    "    used_features = np.append(used_features, tmp_2[Choose_num_of_second_output_parameter - 1])\n",
    "\n",
    "    tmp_1 = np.delete(tmp_1, Choose_num_of_first_output_parameter - 1)\n",
    "    tmp_2 = np.delete(tmp_2, Choose_num_of_first_output_parameter - 1)\n",
    "\n",
    "    tmp_1 = np.delete(tmp_1, Choose_num_of_second_output_parameter - 2)\n",
    "    tmp_2 = np.delete(tmp_2, Choose_num_of_second_output_parameter - 2)\n",
    "\n",
    "    for idx, val in enumerate(tmp_1):\n",
    "        if val == 1:\n",
    "            used_features = np.append(used_features, tmp_2[idx])\n",
    "\n",
    "    name_of_used_features = ''\n",
    "    for name in used_features:\n",
    "        name_of_used_features = name_of_used_features + '_' + name\n",
    "    \n",
    "    print(\"Input features =\", used_features) \n",
    "    print(\"Onput features =\", used_features[0], used_features[1])\n",
    "\n",
    "    return used_features, name_of_used_features, num_of_features\n",
    "    \n",
    "\n",
    "    if not os.path.exists(current_path + '\\\\Model'):\n",
    "        os.makedirs(current_path + '\\\\Model')\n",
    "\n",
    "###  This function creates a sliding window or sequences of 28 days and one day label ####\n",
    "###  For Multiple features                                                            ####\n",
    "# def sliding_windows_mutli_features(data, seq_length):\n",
    "#     x = []\n",
    "#     y = []\n",
    "#     for i in range((data.shape[0])-seq_length-1):\n",
    "#         _x = data[i:(i+seq_length), :] ## 3 columns for features  \n",
    "#         _y = data[i+seq_length, 0:2] ## column 0 contains the labbel\n",
    "#         x.append(_x)\n",
    "#         y.append(_y)\n",
    "\n",
    "#     return np.array(x), np.array(y).reshape(-1, 2)\n",
    "\n",
    "###  This function creates a sliding window or sequences of 28 days and one day label ####\n",
    "###  For Multiple features                                                            ####\n",
    "def sliding_windows_mutli_features(data, seq_length):\n",
    "    x = []\n",
    "    y = []\n",
    "    for i in range((data.shape[0])-seq_length):\n",
    "        _x_prod = data[i:(i+seq_length), 0:2] ## 3 columns for features\n",
    "        _x_oper = data[i+1:(i+seq_length+1), 2:]\n",
    "        _x = np.concatenate((_x_prod, _x_oper), axis=1)  \n",
    "        _y = data[i+seq_length, 0:2] ## column 0 contains the labbel\n",
    "        x.append(_x)\n",
    "        y.append(_y)\n",
    "\n",
    "    return np.array(x), np.array(y).reshape(-1, 2)\n",
    "\n",
    "class LSTM2(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes, input_size, hidden_size, num_layers):\n",
    "        super(LSTM2, self).__init__()\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "        self.num_layers = num_layers\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.batch_size = 1\n",
    "        #self.seq_length = seq_length\n",
    "        \n",
    "        self.LSTM2 = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True, dropout = 0.2)\n",
    "    \n",
    "        self.fc1 = nn.Linear(hidden_size, 256)\n",
    "        self.bn1 = nn.BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.dp1 = nn.Dropout(0.25)\n",
    "        \n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.bn2 = nn.BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.dp2 = nn.Dropout(0.2)\n",
    "\n",
    "        self.fc3= nn.Linear(128, 2)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h_1 = Variable(torch.zeros(\n",
    "            self.num_layers, x.size(0), self.hidden_size).to(device))\n",
    "        c_1 = Variable(torch.zeros(\n",
    "            self.num_layers, x.size(0), self.hidden_size).to(device))\n",
    "        _, (hn, cn) = self.LSTM2(x, (h_1, c_1))\n",
    "    \n",
    "        #print(\"hidden state shpe is:\",hn.size())\n",
    "        y = hn.view(-1, self.hidden_size)\n",
    "        \n",
    "        final_state = hn.view(self.num_layers, x.size(0), self.hidden_size)[-1]\n",
    "        #print(\"final state shape is:\",final_state.shape)\n",
    "        \n",
    "        x0 = self.fc1(final_state)\n",
    "        x0 = self.bn1(x0)\n",
    "        x0 = self.dp1(x0)\n",
    "        x0 = self.relu(x0)\n",
    "        \n",
    "        x0 = self.fc2(x0)\n",
    "        x0 = self.bn2(x0)\n",
    "        x0 = self.dp2(x0)\n",
    "        x0 = self.relu(x0)\n",
    "        \n",
    "        out = self.fc3(x0)\n",
    "        #print(out.size())\n",
    "        return out\n",
    "\n",
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x_data = x\n",
    "        self.y_data = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = Variable(torch.Tensor(self.x_data[idx]))\n",
    "        y = Variable(torch.Tensor(self.y_data[idx]))\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(current_path + '\\\\Results'):\n",
    "        os.makedirs(current_path + '\\\\Results')\n",
    "if not os.path.exists(current_path + '\\\\Results\\\\' + use_datetime):\n",
    "        os.makedirs(current_path + '\\\\Results\\\\' + use_datetime)\n",
    "if not os.path.exists(current_path + '\\\\Figs\\\\' + use_datetime):\n",
    "    os.makedirs(current_path + '\\\\Figs\\\\' + use_datetime)\n",
    "if not os.path.exists(current_path + '\\\\Figs\\\\' + use_datetime + '\\\\TestData'):\n",
    "    os.makedirs(current_path + '\\\\Figs\\\\' + use_datetime + '\\\\TestData')\n",
    "if not os.path.exists(current_path + '\\\\Figs\\\\' + use_datetime + '\\\\TestData\\\\Prod'):\n",
    "    os.makedirs(current_path + '\\\\Figs\\\\' + use_datetime + '\\\\TestData\\\\Prod')\n",
    "if not os.path.exists(current_path + '\\\\Figs\\\\' + use_datetime + '\\\\TestData\\\\CumProd'):\n",
    "    os.makedirs(current_path + '\\\\Figs\\\\' + use_datetime + '\\\\TestData\\\\CumProd')\n",
    "if not os.path.exists(current_path + '\\\\Figs\\\\' + use_datetime + '\\\\TestData\\\\ProdbyCum'):\n",
    "    os.makedirs(current_path + '\\\\Figs\\\\' + use_datetime + '\\\\TestData\\\\ProdbyCum')\n",
    "\n",
    "pd.set_option('max_columns', 50)\n",
    "plt.style.use('bmh')\n",
    "color_pal = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "color_cycle = cycle(plt.rcParams['axes.prop_cycle'].by_key()['color'])\n",
    "\n",
    "ngpu = 1\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "# current_path = os.getcwd()\n",
    "\n",
    "if not os.path.exists(current_path + '\\\\Model'):\n",
    "        os.makedirs(current_path + '\\\\Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input features = ['GasProd_MCF' 'CumGas_MCF' 'ProducingDays' 'CumProdDay' 'ShutinMonths'\n",
      " 'Refrac']\n",
      "Onput features = GasProd_MCF CumGas_MCF\n"
     ]
    }
   ],
   "source": [
    "########################### Start of Control Pannel ###########################\n",
    "\"\"\"\n",
    "    - It should be same with the number of num_features and sum of use_or_not_features.\n",
    "    - Ex. use_or_not_CumMonth = 1 means you will use that parameter.\n",
    "\"\"\"\n",
    "use_or_not_ProdBOE    = 0    # Ex. 3000, 1000, 500, ...\n",
    "use_or_not_DailyRate  = 0    # Ex. 100, 90, 40, ...  = ProdBOE/ProdDays\n",
    "use_or_not_CumProdBOE = 0    # Ex. 3000, 4000, 4500, ...\n",
    "use_or_not_OilProd    = 0\n",
    "use_or_not_OilRate    = 0\n",
    "use_or_not_CumOil     = 0\n",
    "use_or_not_GasProd    = 1\n",
    "use_or_not_GasRate    = 0\n",
    "use_or_not_CumGas     = 1\n",
    "use_or_not_WaterProd  = 0 \n",
    "\n",
    "use_or_not_CumMonth   = 0    # Ex. 1, 2, 3, 4, ...\n",
    "use_or_not_ProdDays   = 1    # Ex. 30, 31, 29, 5, ...\n",
    "use_or_not_CumProdDay = 1      \n",
    "use_or_not_Shutin     = 1    # Ex. 0, 1, 0, 0, 2, 0, ...\n",
    "use_or_not_Refrac     = 1\n",
    "\n",
    "Choose_num_of_output_parameter = 2\n",
    "Choose_num_of_first_output_parameter = 7\n",
    "Choose_num_of_second_output_parameter = 9\n",
    "\n",
    "########################### End of Control Pannel ###########################\n",
    "\n",
    "used_features, name_of_used_features, num_of_features = Set_Var(Choose_num_of_first_output_parameter, Choose_num_of_second_output_parameter, use_or_not_ProdBOE, use_or_not_DailyRate, use_or_not_CumProdBOE, \n",
    "                                                                            use_or_not_OilProd, use_or_not_OilRate, use_or_not_CumOil, use_or_not_GasProd, use_or_not_GasRate, use_or_not_CumGas, \n",
    "                                                                            use_or_not_WaterProd, use_or_not_CumMonth, use_or_not_ProdDays, use_or_not_CumProdDay, use_or_not_Shutin, use_or_not_Refrac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Start of Control Pannel #########################\n",
    "test_input_path = current_path + '\\\\Data\\\\After_Preprocess\\\\' + use_datetime + '\\\\Test\\\\' ## Refrac만 Testdataset으로 할 때 해당 경로를 바꿔주면 됨\n",
    "use_datetime = 'BaseCase' # Check the data in the Data dictionary.\n",
    "seq_length = 3\n",
    "seq_length_string = '3'\n",
    "used_seq = 3\n",
    "use_or_not_EUR_until_Last_months = 1 ## if 1 --> EUR_standard set by last months automatically. and then, below value(EUR standard) is no used.\n",
    "used_period_lst = [3, 6, 9] # used_period는 seq_length보다 짧아선 안됨. min of used_period = seq_length\n",
    "used_period = used_period_lst[0] # Best, Worst Case 그릴때 얼마까지 사용해서 그릴것인지 결정\n",
    "EUR_standard = 60\n",
    "choose_scaler = 2  # 1=standard, 2=minmax, 3=Robust, 4=QuantileTransformer, 5=PowerTransformer\n",
    "lamda = 1 # 여기서는 prediction 할 때 EUR을 계산하는 비율을 결정하는걸로, 학습시켰을 때 비랑 일정하게 하려고 함.\n",
    "############################## Hyperparameters ##############################\n",
    "learning_rate = 1e-3\n",
    "hidden_size = 128\n",
    "num_layers = 3\n",
    "num_classes = 1\n",
    "########################### End of Control Pannel ###########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7121668\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'cpu'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-f285057a5ec5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    100\u001b[0m             \u001b[0mdata_predict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_predict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_predict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m             \u001b[0mprod_for_compare\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mY_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m             \u001b[0mprod_for_compare_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscaler_prod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprod_for_compare\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprod_for_compare_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'cpu'"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "Code for checking Result\n",
    "\"\"\"\n",
    "\n",
    "input_size = num_of_features\n",
    "\n",
    "lstm = LSTM2(num_classes, input_size, hidden_size, num_layers)\n",
    "lstm.to(device)\n",
    "\n",
    "lstm.apply(init_weights)\n",
    "\n",
    "criterion = torch.nn.MSELoss().to(device)    # mean-squared error for regression\n",
    "optimizer = torch.optim.Adam(lstm.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=50, factor =0.5, min_lr=1e-7, eps=1e-08)\n",
    "\n",
    "scaler_prod = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler_cumprod = MinMaxScaler(feature_range=(0, 0.0375))\n",
    "scaler_oper = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "test_input_path = current_path + '\\\\Data\\\\After_Preprocess\\\\' + use_datetime + '\\\\Test\\\\' ## Refrac만 Testdataset으로 할 때 해당 경로를 바꿔주면 됨\n",
    "test_file_list = os.listdir(test_input_path)\n",
    "\n",
    "for idx_, val in enumerate(used_period_lst):\n",
    "    \"\"\"\n",
    "    - Control Pannel\n",
    "    - Set the parameters\n",
    "    \"\"\"\n",
    "    used_period = val   # used_period는 seq_length보다 짧아선 안됨. min of used_period = seq_length\n",
    "    pred_period = EUR_standard - used_period\n",
    "\n",
    "    #########################################\n",
    "\n",
    "    EUR_true_lst = []\n",
    "    EUR_pred_lst = []\n",
    "    EUR_true_lst_cum = []\n",
    "    EUR_pred_lst_cum = []\n",
    "    EUR_pred_lst_comb = []\n",
    "    EUR_true_lst_prodbycum = []\n",
    "    EUR_pred_lst_prodbycum = []\n",
    "    Wellname_lst = []\n",
    "\n",
    "    np.random.shuffle(test_file_list)\n",
    "    for idx in range(len(test_file_list)):\n",
    "        X = []\n",
    "        Y = []\n",
    "        EUR_True = 0\n",
    "        EUR_Pred = 0\n",
    "        EUR_sum = 0\n",
    "\n",
    "        name_of_data = test_file_list[idx]\n",
    "        df_temp = pd.read_csv(test_input_path + name_of_data)\n",
    "        df_TEST = pd.DataFrame(columns=used_features)\n",
    "\n",
    "        if use_or_not_EUR_until_Last_months == 1:\n",
    "            EUR_standard = len(df_temp)\n",
    "            pred_period = EUR_standard - used_period\n",
    "        else: pass\n",
    "        \n",
    "        ## 처음부터 데이터 가져올 때, 생산량 외의 정보는 t+1 시점으로 맞춰줘야함\n",
    "        for x in used_features:\n",
    "            df_TEST[x] = df_temp[x]\n",
    "        data = np.array(df_TEST)\n",
    "        origin_data = data[:used_period, 0]\n",
    "        origin_data_cum = data[:used_period, 1]\n",
    "        scaler_prod.fit(data[:used_seq, 0].reshape(-1, 1))\n",
    "        scaler_cumprod.fit(data[:used_seq, 1].reshape(-1, 1))\n",
    "        scaler_oper.fit(data[:, 2:])\n",
    "\n",
    "        scaled_prod = scaler_prod.transform(data[:, 0].reshape(-1, 1))\n",
    "        scaled_cumprod = scaler_cumprod.transform(data[:, 1].reshape(-1, 1))\n",
    "        scaled_oper = scaler_oper.transform(data[:, 2:])\n",
    "\n",
    "        train_data_scaled = np.concatenate((scaled_prod, scaled_cumprod), axis=1)\n",
    "        train_data_scaled = np.concatenate((train_data_scaled, scaled_oper), axis=1)\n",
    "\n",
    "        test_data_normalized = train_data_scaled\n",
    "\n",
    "        if seq_length <= used_period:\n",
    "            X_test_prod = test_data_normalized[used_period - seq_length:used_period, 0:2]\n",
    "            X_test_oper = test_data_normalized[used_period - seq_length+1:used_period+1, 2:]\n",
    "            X_test = np.concatenate((X_test_prod, X_test_oper), axis=1) \n",
    "            Y_test = test_data_normalized[used_period:used_period + pred_period, :]\n",
    "        else:\n",
    "            print(\" used_period is shorter than the length of seq_length, it should be longer.\")\n",
    "\n",
    "        X_test = np.reshape(X_test, (-1, seq_length, num_of_features))\n",
    "        Y_test = np.reshape(Y_test, (-1, pred_period, num_of_features))\n",
    "\n",
    "        Xtest = Variable(torch.Tensor(np.array(X_test)))\n",
    "        Ytest = Variable(torch.Tensor(np.array(Y_test)))\n",
    "        Ytest_plot = Ytest.data.numpy()\n",
    "\n",
    "        predict = []\n",
    "        Xtest_tmp = Xtest\n",
    "\n",
    "        for pred in range(pred_period):\n",
    "            lstm.load_state_dict(torch.load(current_path + '\\\\Model\\\\Best_model_Case_' + name_of_used_features + '_' + str(seq_length) + 'seq_' + use_datetime + '_Test' + '.pt'))\n",
    "            lstm.eval()\n",
    "            test_predict = lstm(Xtest_tmp.to(device)) ## seq_len 넣어서 다음거 하나 나옴, 다시 얘를 포함해서 생산량(Output이랑 동일) 6개 맞추고, 나머지 차원애들은 참값을 불러와줘야함.\n",
    "            data_predict = test_predict.cpu().data.numpy()\n",
    "            print(data_predict[0][0])\n",
    "            prod_for_compare = Y_test.cpu().data.numpy()[0][2, 0]\n",
    "            prod_for_compare_ = scaler_prod.inverse_transform(prod_for_compare.reshape(-1, 1))\n",
    "            print(prod_for_compare_)\n",
    "            cascade_lst = []\n",
    "            cascade_lst = np.append(cascade_lst, data_predict[0][0])\n",
    "            cascade_lst = np.append(cascade_lst, data_predict[0][1])\n",
    "            \n",
    "            if pred < pred_period-1:\n",
    "                for idx in range(num_of_features-2):\n",
    "                    cascade_lst = np.append(cascade_lst, Ytest[:, pred+1, idx+2])\n",
    "\n",
    "                predict = np.append(predict, np.array(cascade_lst))\n",
    "                Xtest_tmp = np.append(Xtest_tmp, cascade_lst)\n",
    "                Xtest_tmp = np.reshape(Xtest_tmp, (-1, seq_length+1, num_of_features))\n",
    "                Xtest_tmp = Xtest_tmp[:, 1:, :]\n",
    "                Xtest_tmp = Variable(torch.Tensor(np.array(Xtest_tmp)))\n",
    "\n",
    "        for idx in range(num_of_features-2):\n",
    "            cascade_lst = np.append(cascade_lst, Ytest[:, pred, idx+2])\n",
    "\n",
    "        predict = np.append(predict, np.array(cascade_lst))\n",
    "        predict = np.reshape(predict, (-1, pred_period, num_of_features))\n",
    "\n",
    "        # Inverse Normalizaiton\n",
    "        predict = scaler.inverse_transform(predict.reshape(-1, num_of_features))\n",
    "        true = scaler.inverse_transform(Ytest_plot.reshape(-1, num_of_features))\n",
    "\n",
    "        # Cal EUR by Prod\n",
    "        data_plot_true = np.append(origin_data, true[:, 0])\n",
    "        data_plot_pred = np.append(origin_data, predict[:, 0])\n",
    "        name = name_of_data.strip(\".csv\")\n",
    "\n",
    "        EUR_sum = EUR_sum + np.abs(np.sum(predict[:, 0]) - np.sum(true[:, 0]))\n",
    "        EUR_true = np.sum(true[:, 0]) + np.sum(origin_data)\n",
    "        EUR_pred = np.sum(predict[:, 0]) + np.sum(origin_data)\n",
    "\n",
    "        EUR_true_lst = np.append(EUR_true_lst, EUR_true)\n",
    "        EUR_pred_lst = np.append(EUR_pred_lst, EUR_pred)\n",
    "        Wellname_lst = np.append(Wellname_lst, name)\n",
    "\n",
    "        # Cal EUR by CumProd\n",
    "        data_plot_true_cum = np.append(origin_data_cum, true[:, 1])\n",
    "        data_plot_pred_cum = np.append(origin_data_cum, predict[:, 1])\n",
    "\n",
    "        EUR_true_cum = data_plot_true_cum[-1]\n",
    "        EUR_pred_cum = data_plot_pred_cum[-1]\n",
    "\n",
    "        EUR_true_lst_cum = np.append(EUR_true_lst_cum, EUR_true_cum)\n",
    "        EUR_pred_lst_cum = np.append(EUR_pred_lst_cum, EUR_pred_cum)\n",
    "\n",
    "        # Cal EUR by combination with Prod and CumProd by weight for Learning\n",
    "        EUR_pred_lst_comb = lamda * EUR_pred_lst + (1-lamda) * EUR_pred_lst_cum\n",
    "\n",
    "        # Cal EUR by Prod by Cum\n",
    "        data_plot_true_prodbycum = np.zeros(len(data_plot_true_cum))\n",
    "        data_plot_pred_prodbycum = np.zeros(len(data_plot_pred_cum))\n",
    "        origin_data_prodbycum = np.zeros(len(origin_data_cum))\n",
    "\n",
    "        for idx, value in enumerate(data_plot_true_cum):\n",
    "            if idx == 0:\n",
    "                data_plot_true_prodbycum[idx] = value\n",
    "            else:\n",
    "                data_plot_true_prodbycum[idx] = (value - data_plot_true_cum[idx-1])\n",
    "\n",
    "        for idx, value in enumerate(data_plot_pred_cum):\n",
    "            if idx == 0:\n",
    "                data_plot_pred_prodbycum[idx] = value\n",
    "            else:\n",
    "                data_plot_pred_prodbycum[idx] = (value - data_plot_pred_cum[idx-1])\n",
    "\n",
    "        for idx, value in enumerate(origin_data_cum):\n",
    "            if idx == 0:\n",
    "                origin_data_prodbycum[idx] = value\n",
    "            else:\n",
    "                origin_data_prodbycum[idx] = (value - origin_data_cum[idx-1])\n",
    "\n",
    "        # EUR_sum = EUR_sum + np.abs(np.sum(predict[:, 0]) - np.sum(true[:, 0]))\n",
    "        EUR_true_prodbycum = np.sum(data_plot_true_prodbycum) + np.sum(origin_data_prodbycum)\n",
    "        EUR_pred_prodbycum = np.sum(data_plot_pred_prodbycum) + np.sum(origin_data_prodbycum)\n",
    "\n",
    "        EUR_true_lst_prodbycum = np.append(EUR_true_lst_prodbycum, EUR_true_prodbycum)\n",
    "        EUR_pred_lst_prodbycum = np.append(EUR_pred_lst_prodbycum, EUR_pred_prodbycum)\n",
    "\n",
    "        if used_period == 3:\n",
    "            # Plot for Prod\n",
    "            figure(num=None, figsize=(19, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "            plt.plot(data_plot_true, 'b')\n",
    "            plt.plot(data_plot_pred, 'r')\n",
    "            plt.plot(origin_data, 'k')\n",
    "            plt.legend(['True', 'Prediction'], fontsize=21)\n",
    "            plt.suptitle(name, fontsize=23)\n",
    "            plt.xticks(fontsize=21)\n",
    "            plt.yticks(fontsize=21)\n",
    "            plt.ylabel(ylabel = 'Production', fontsize=21)\n",
    "            plt.xlabel(xlabel = 'Month', fontsize=21)\n",
    "            plt.savefig(current_path + '\\\\Figs\\\\' + use_datetime + '\\\\TestData\\\\Prod\\\\' + name + '.png')\n",
    "            plt.close()\n",
    "\n",
    "            # Plot for CumProd\n",
    "            figure(num=None, figsize=(19, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "            plt.plot(data_plot_true_cum, 'b')\n",
    "            plt.plot(data_plot_pred_cum, 'r')\n",
    "            plt.plot(origin_data_cum, 'k')\n",
    "            plt.legend(['True', 'Prediction'], fontsize=21)\n",
    "            plt.suptitle(name, fontsize=23)\n",
    "            plt.xticks(fontsize=21)\n",
    "            plt.yticks(fontsize=21)\n",
    "            plt.ylabel(ylabel = 'Production', fontsize=21)\n",
    "            plt.xlabel(xlabel = 'Month', fontsize=21)\n",
    "            plt.savefig(current_path + '\\\\Figs\\\\' + use_datetime + '\\\\TestData\\\\CumProd\\\\' + name + '.png')\n",
    "            plt.close()\n",
    "\n",
    "            # Plot for ProdbyCum            \n",
    "            figure(num=None, figsize=(19, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "            plt.plot(data_plot_true_prodbycum, 'b')\n",
    "            plt.plot(data_plot_pred_prodbycum, 'r')\n",
    "            plt.plot(origin_data_prodbycum, 'k')\n",
    "            plt.legend(['True', 'Prediction'], fontsize=21)\n",
    "            plt.suptitle(name, fontsize=23)\n",
    "            plt.xticks(fontsize=21)\n",
    "            plt.yticks(fontsize=21)\n",
    "            plt.ylabel(ylabel = 'Production', fontsize=21)\n",
    "            plt.xlabel(xlabel = 'Month', fontsize=21)\n",
    "            plt.savefig(current_path + '\\\\Figs\\\\' + use_datetime + '\\\\TestData\\\\ProdbyCum\\\\' + name + '.png')\n",
    "            plt.close()\n",
    "\n",
    "    colums_EUR = ['name', 'True', 'Pred']\n",
    "    df_EUR = pd.DataFrame(columns=colums_EUR)\n",
    "    df_EUR['name'] = Wellname_lst\n",
    "    df_EUR['True'] = EUR_true_lst\n",
    "    df_EUR['Pred'] = EUR_pred_lst\n",
    "    df_EUR.to_csv(current_path + '/Results/' + use_datetime + '/EUR_scatterplot_' + str(val) + '.csv')\n",
    "\n",
    "    df_EUR_cum = pd.DataFrame(columns=colums_EUR)\n",
    "    df_EUR_cum['name'] = Wellname_lst\n",
    "    df_EUR_cum['True'] = EUR_true_lst_cum\n",
    "    df_EUR_cum['Pred'] = EUR_pred_lst_cum\n",
    "    df_EUR_cum.to_csv(current_path + '/Results/' + use_datetime + '/EUR_scatterplot_cum_' + str(val) + '.csv')\n",
    "\n",
    "    df_EUR_comb = pd.DataFrame(columns=colums_EUR)\n",
    "    df_EUR_comb['name'] = Wellname_lst\n",
    "    df_EUR_comb['True'] = EUR_true_lst_cum\n",
    "    df_EUR_comb['Pred'] = EUR_pred_lst_comb\n",
    "    df_EUR_comb.to_csv(current_path + '/Results/' + use_datetime + '/EUR_scatterplot_comb_' + str(val) + '.csv')\n",
    "\n",
    "    df_EUR_prodbycum = pd.DataFrame(columns=colums_EUR)\n",
    "    df_EUR_prodbycum['name'] = Wellname_lst\n",
    "    df_EUR_prodbycum['True'] = EUR_true_lst_prodbycum\n",
    "    df_EUR_prodbycum['Pred'] = EUR_pred_lst_prodbycum\n",
    "    df_EUR_prodbycum.to_csv(current_path + '/Results/' + use_datetime + '/EUR_scatterplot_prodbycum_' + str(val) + '.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('Joko')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fa1c66b4b9bc522ad91ca04eeae32ed2414efe300d04b71e2b70e93931d737f8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
