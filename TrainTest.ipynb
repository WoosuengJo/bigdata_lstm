{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import gc\n",
    "import datetime as dt\n",
    "from datetime import datetime, timedelta \n",
    "from openpyxl import load_workbook\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "from sklearn import preprocessing, metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import dask.dataframe as dd\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "import seaborn as sns\n",
    "import collections as co\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import lightgbm as lgb\n",
    "from ipywidgets import widgets, interactive\n",
    "from typing import Union\n",
    "from tqdm.notebook import tqdm_notebook as tqdm\n",
    "from itertools import cycle\n",
    "from fastprogress import master_bar, progress_bar\n",
    "\n",
    "pd.set_option('max_columns', 50)\n",
    "plt.style.use('bmh')\n",
    "color_pal = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "color_cycle = cycle(plt.rcParams['axes.prop_cycle'].by_key()['color'])\n",
    "\n",
    "ngpu = 1\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "current_path = os.getcwd()\n",
    "\n",
    "def Set_Var(Choose_num_of_first_output_parameter, Choose_num_of_second_output_parameter, use_or_not_ProdBOE, use_or_not_DailyRate, use_or_not_CumProdBOE, \n",
    "use_or_not_OilProd, use_or_not_OilRate, use_or_not_CumOil, use_or_not_GasProd, use_or_not_GasRate, use_or_not_CumGas, \n",
    "use_or_not_WaterProd, use_or_not_CumMonth, use_or_not_ProdDays, use_or_not_CumProdDay, use_or_not_Shutin, use_or_not_Refrac):\n",
    "\n",
    "    num_of_features = use_or_not_ProdBOE + use_or_not_DailyRate + use_or_not_CumProdBOE + use_or_not_OilProd + use_or_not_OilRate + use_or_not_CumOil + use_or_not_GasProd + use_or_not_GasRate + use_or_not_CumGas + use_or_not_WaterProd + use_or_not_CumMonth + use_or_not_ProdDays + use_or_not_CumProdDay + use_or_not_Shutin + use_or_not_Refrac\n",
    "    tmp_1 = [use_or_not_ProdBOE, use_or_not_DailyRate, use_or_not_CumProdBOE, use_or_not_OilProd, use_or_not_OilRate, use_or_not_CumOil, use_or_not_GasProd, use_or_not_GasRate, use_or_not_CumGas, use_or_not_WaterProd, use_or_not_CumMonth, use_or_not_ProdDays, use_or_not_CumProdDay, use_or_not_Shutin, use_or_not_Refrac]\n",
    "    tmp_2 = [\"Prod_BOE\", \"ProdRate_BOE\", \"CumProd_BOE\", 'LiquidsProd_BBL', 'Liq_rate', 'CumLiquids_BBL', 'GasProd_MCF', 'Gas_rate','CumGas_MCF', 'WaterProd_BBL', \"TotalProdMonths\", \"ProducingDays\", 'CumProdDay', \"ShutinMonths\", \"Refrac\"]\n",
    "    used_features = []\n",
    "\n",
    "    used_features = np.append(used_features, tmp_2[Choose_num_of_first_output_parameter - 1])\n",
    "    used_features = np.append(used_features, tmp_2[Choose_num_of_second_output_parameter - 1])\n",
    "\n",
    "    tmp_1 = np.delete(tmp_1, Choose_num_of_first_output_parameter - 1)\n",
    "    tmp_2 = np.delete(tmp_2, Choose_num_of_first_output_parameter - 1)\n",
    "\n",
    "    tmp_1 = np.delete(tmp_1, Choose_num_of_second_output_parameter - 2)\n",
    "    tmp_2 = np.delete(tmp_2, Choose_num_of_second_output_parameter - 2)\n",
    "\n",
    "    for idx, val in enumerate(tmp_1):\n",
    "        if val == 1:\n",
    "            used_features = np.append(used_features, tmp_2[idx])\n",
    "\n",
    "    name_of_used_features = ''\n",
    "    for name in used_features:\n",
    "        name_of_used_features = name_of_used_features + '_' + name\n",
    "    \n",
    "    print(\"Input features =\", used_features) \n",
    "    print(\"Onput features =\", used_features[0], used_features[1])\n",
    "\n",
    "    return used_features, name_of_used_features, num_of_features\n",
    "    \n",
    "\n",
    "    if not os.path.exists(current_path + '\\\\Model'):\n",
    "        os.makedirs(current_path + '\\\\Model')\n",
    "\n",
    "###  This function creates a sliding window or sequences of 28 days and one day label ####\n",
    "###  For Multiple features                                                            ####\n",
    "# def sliding_windows_mutli_features(data, seq_length):\n",
    "#     x = []\n",
    "#     y = []\n",
    "#     for i in range((data.shape[0])-seq_length-1):\n",
    "#         _x = data[i:(i+seq_length), :] ## 3 columns for features  \n",
    "#         _y = data[i+seq_length, 0:2] ## column 0 contains the labbel\n",
    "#         x.append(_x)\n",
    "#         y.append(_y)\n",
    "\n",
    "#     return np.array(x), np.array(y).reshape(-1, 2)\n",
    "\n",
    "###  This function creates a sliding window or sequences of 28 days and one day label ####\n",
    "###  For Multiple features                                                            ####\n",
    "def sliding_windows_mutli_features(data, seq_length):\n",
    "    x = []\n",
    "    y = []\n",
    "    for i in range((data.shape[0])-seq_length):\n",
    "        _x_prod = data[i:(i+seq_length), 0:2] ## 3 columns for features\n",
    "        _x_oper = data[i+1:(i+seq_length+1), 2:]\n",
    "        _x = np.concatenate((_x_prod, _x_oper), axis=1)  \n",
    "        _y = data[i+seq_length, 0:2] ## column 0 contains the labbel\n",
    "        x.append(_x)\n",
    "        y.append(_y)\n",
    "\n",
    "    return np.array(x), np.array(y).reshape(-1, 2)\n",
    "\n",
    "class LSTM2(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes, input_size, hidden_size, num_layers):\n",
    "        super(LSTM2, self).__init__()\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "        self.num_layers = num_layers\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.batch_size = 1\n",
    "        #self.seq_length = seq_length\n",
    "        \n",
    "        self.LSTM2 = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True, dropout = 0.2)\n",
    "    \n",
    "        self.fc1 = nn.Linear(hidden_size, 256)\n",
    "        self.bn1 = nn.BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.dp1 = nn.Dropout(0.25)\n",
    "        \n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.bn2 = nn.BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.dp2 = nn.Dropout(0.2)\n",
    "\n",
    "        self.fc3= nn.Linear(128, 2)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h_1 = Variable(torch.zeros(\n",
    "            self.num_layers, x.size(0), self.hidden_size).to(device))\n",
    "        c_1 = Variable(torch.zeros(\n",
    "            self.num_layers, x.size(0), self.hidden_size).to(device))\n",
    "        _, (hn, cn) = self.LSTM2(x, (h_1, c_1))\n",
    "    \n",
    "        #print(\"hidden state shpe is:\",hn.size())\n",
    "        y = hn.view(-1, self.hidden_size)\n",
    "        \n",
    "        final_state = hn.view(self.num_layers, x.size(0), self.hidden_size)[-1]\n",
    "        #print(\"final state shape is:\",final_state.shape)\n",
    "        \n",
    "        x0 = self.fc1(final_state)\n",
    "        x0 = self.bn1(x0)\n",
    "        x0 = self.dp1(x0)\n",
    "        x0 = self.relu(x0)\n",
    "        \n",
    "        x0 = self.fc2(x0)\n",
    "        x0 = self.bn2(x0)\n",
    "        x0 = self.dp2(x0)\n",
    "        x0 = self.relu(x0)\n",
    "        \n",
    "        out = self.fc3(x0)\n",
    "        #print(out.size())\n",
    "        return out\n",
    "\n",
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x_data = x\n",
    "        self.y_data = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = Variable(torch.Tensor(self.x_data[idx]))\n",
    "        y = Variable(torch.Tensor(self.y_data[idx]))\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input features = ['GasProd_MCF' 'CumGas_MCF' 'ProducingDays' 'CumProdDay' 'ShutinMonths'\n",
      " 'Refrac']\n",
      "Onput features = GasProd_MCF CumGas_MCF\n"
     ]
    }
   ],
   "source": [
    "########################### Start of Control Pannel ###########################\n",
    "\"\"\"\n",
    "    - It should be same with the number of num_features and sum of use_or_not_features.\n",
    "    - Ex. use_or_not_CumMonth = 1 means you will use that parameter.\n",
    "\"\"\"\n",
    "use_or_not_ProdBOE    = 0    # Ex. 3000, 1000, 500, ...\n",
    "use_or_not_DailyRate  = 0    # Ex. 100, 90, 40, ...  = ProdBOE/ProdDays\n",
    "use_or_not_CumProdBOE = 0    # Ex. 3000, 4000, 4500, ...\n",
    "use_or_not_OilProd    = 0\n",
    "use_or_not_OilRate    = 0\n",
    "use_or_not_CumOil     = 0\n",
    "use_or_not_GasProd    = 1\n",
    "use_or_not_GasRate    = 0\n",
    "use_or_not_CumGas     = 1\n",
    "use_or_not_WaterProd  = 0 \n",
    "\n",
    "use_or_not_CumMonth   = 0    # Ex. 1, 2, 3, 4, ...\n",
    "use_or_not_ProdDays   = 1    # Ex. 30, 31, 29, 5, ...\n",
    "use_or_not_CumProdDay = 1      \n",
    "use_or_not_Shutin     = 1    # Ex. 0, 1, 0, 0, 2, 0, ...\n",
    "use_or_not_Refrac     = 1\n",
    "\n",
    "Choose_num_of_output_parameter = 2\n",
    "Choose_num_of_first_output_parameter = 7\n",
    "Choose_num_of_second_output_parameter = 9\n",
    "\n",
    "########################### End of Control Pannel ###########################\n",
    "\n",
    "used_features, name_of_used_features, num_of_features = Set_Var(Choose_num_of_first_output_parameter, Choose_num_of_second_output_parameter, use_or_not_ProdBOE, use_or_not_DailyRate, use_or_not_CumProdBOE, \n",
    "                                                                            use_or_not_OilProd, use_or_not_OilRate, use_or_not_CumOil, use_or_not_GasProd, use_or_not_GasRate, use_or_not_CumGas, \n",
    "                                                                            use_or_not_WaterProd, use_or_not_CumMonth, use_or_not_ProdDays, use_or_not_CumProdDay, use_or_not_Shutin, use_or_not_Refrac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_datetime = 'BaseCase' # Check the data in the Data dictionary.\n",
    "seq_length = 3\n",
    "lamda = 0 ### 생산량과 누적생산량 맞추기 비율, 앞에 lamda 붙어있음 (확인 다시 필요)\n",
    "choose_scaler = 1 # 1=standard, 2=minmax, 3=Robust, 4=QuantileTransformer, 5=PowerTransformer\n",
    "num_of_out_features = 2\n",
    "seed = 1\n",
    "############################## Hyperparameters ##############################\n",
    "batch_size = 128 # 128\n",
    "num_epochs = 100\n",
    "learning_rate = 1e-2 # 1e-3\n",
    "hidden_size = 128\n",
    "num_layers = 3\n",
    "num_classes = 1\n",
    "best_val_loss = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Train / Valid / Test Dataset 만들기, Refrac 데이터도 각 비율 만큼 넣기\n",
    "\"\"\"\n",
    "input_path_train = current_path + '\\\\Data\\\\After_Preprocess\\\\' + use_datetime + '\\\\Pre_Train\\\\'\n",
    "pre_train_file_list = os.listdir(input_path_train)\n",
    "\n",
    "input_path_train_refrac = current_path + '\\\\Data\\\\After_Preprocess\\\\' + use_datetime + '\\\\Pre_Refrac\\\\Train\\\\'\n",
    "pre_train_file_list_refrac = os.listdir(input_path_train_refrac)\n",
    "\n",
    "if not os.path.exists(current_path + '\\\\Data\\\\After_Preprocess\\\\' + use_datetime + '\\\\Train\\\\'):\n",
    "        os.makedirs(current_path + '\\\\Data\\\\After_Preprocess\\\\' + use_datetime + '\\\\Train\\\\')\n",
    "\n",
    "if not os.path.exists(current_path + '\\\\Data\\\\After_Preprocess\\\\' + use_datetime + '\\\\Valid\\\\'):\n",
    "        os.makedirs(current_path + '\\\\Data\\\\After_Preprocess\\\\' + use_datetime + '\\\\Valid\\\\')\n",
    "\n",
    "train_input_path = current_path + '\\\\Data\\\\After_Preprocess\\\\' + use_datetime + '\\\\Train\\\\'\n",
    "valid_input_path = current_path + '\\\\Data\\\\After_Preprocess\\\\' + use_datetime + '\\\\Valid\\\\'\n",
    "\n",
    "seed = 2\n",
    "np.random.seed(seed)\n",
    "np.random.shuffle(pre_train_file_list)\n",
    "np.random.shuffle(pre_train_file_list_refrac)\n",
    "\n",
    "train_size = int(len(pre_train_file_list) * 0.8235)\n",
    "train_size_refrac = int(len(pre_train_file_list_refrac) * 0.8235)\n",
    "\n",
    "for file in pre_train_file_list[0:train_size]:\n",
    "    df = pd.read_csv(input_path_train + file)\n",
    "    df.to_csv(train_input_path + file)\n",
    "\n",
    "for file in pre_train_file_list[train_size:]:\n",
    "    df = pd.read_csv(input_path_train + file)\n",
    "    df.to_csv(valid_input_path + file)   \n",
    "\n",
    "for file in pre_train_file_list_refrac[0:train_size_refrac]:\n",
    "    df = pd.read_csv(input_path_train_refrac + file)\n",
    "    df.to_csv(train_input_path + file)\n",
    "\n",
    "for file in pre_train_file_list_refrac[train_size_refrac:]:\n",
    "    df = pd.read_csv(input_path_train_refrac + file)\n",
    "    df.to_csv(valid_input_path + file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Scale을 Feature 단위로 할 때\n",
    "\"\"\"\n",
    "# df_tot_tmp = pd.DataFrame()\n",
    "# df_tot = pd.DataFrame(columns=used_features)\n",
    "# for file in scale_input_list:\n",
    "#     df_tmp = pd.read_csv(scale_input_path + file)\n",
    "#     df_tot_tmp = pd.concat([df_tot_tmp, df_tmp])\n",
    "\n",
    "# print(df_tmp.shape)\n",
    "# print(df_tot_tmp.shape)\n",
    "\n",
    "# for x in used_features:\n",
    "#     df_tot[x] = df_tot_tmp[x]\n",
    "# data_for_scale = np.array(df_tot)\n",
    "\n",
    "# print(len(used_features), used_features)\n",
    "# display(df_tot.head(10))\n",
    "# display(data_for_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['GasProd_MCF', 'CumGas_MCF', 'ProducingDays', 'CumProdDay',\n",
       "       'ShutinMonths', 'Refrac'], dtype='<U32')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "used_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_prod = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler_cumprod = MinMaxScaler(feature_range=(0, 0.0375))\n",
    "scaler_oper = MinMaxScaler(feature_range=(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReturnDataLoader(path, filename, used_seq):\n",
    "    df_temp = pd.read_csv(path + filename)\n",
    "    df = pd.DataFrame(columns=used_features)\n",
    "    for x in used_features:\n",
    "        df[x] = df_temp[[x]]\n",
    "    data = np.array(df)\n",
    "    scaler_prod.fit(data[:used_seq, 0].reshape(-1, 1))\n",
    "    scaler_cumprod.fit(data[:used_seq, 1].reshape(-1, 1))\n",
    "    scaler_oper.fit(data[:, 2:])\n",
    "\n",
    "    scaled_prod = scaler_prod.transform(data[:, 0].reshape(-1, 1))\n",
    "    scaled_cumprod = scaler_cumprod.transform(data[:, 1].reshape(-1, 1))\n",
    "    scaled_oper = scaler_oper.transform(data[:, 2:])\n",
    "\n",
    "    train_data_scaled = np.concatenate((scaled_prod, scaled_cumprod), axis=1)\n",
    "    train_data_scaled = np.concatenate((train_data_scaled, scaled_oper), axis=1)\n",
    "\n",
    "    x, y = sliding_windows_mutli_features(train_data_scaled, seq_length)\n",
    "    trainX = torch.Tensor(x)\n",
    "    trainY = torch.Tensor(y)\n",
    "    train_dataset = CustomDataset(trainX, trainY)\n",
    "    batch_size = int(len(trainX))\n",
    "    dataloader = DataLoader(train_dataset, batch_size, shuffle=False)\n",
    "\n",
    "    return dataloader\n",
    "\n",
    "def Cal_Restriction(outputs):\n",
    "    t_2 = (outputs[:, 1].cpu().detach().numpy())\n",
    "    t_1 = np.zeros(len(t_2))\n",
    "    t_1[1:] = t_2[0:-1]\n",
    "    zero_ = np.zeros(len(t_2))\n",
    "    pred_ = t_2 - t_1\n",
    "    res_lst = np.minimum(zero_, pred_)\n",
    "    restriction = (np.sum(np.abs(res_lst)))\n",
    "\n",
    "    return restriction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='15' class='' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      15.00% [15/100 03:30<19:50]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved best model epoch: 0 val loss is: 26518.35540607248 train loss is 4.5549774169921875\n",
      "Epoch: 0, loss: 4.55498 valid loss:  26518.35541 \n",
      "Saved best model epoch: 1 val loss is: 26494.411539974502 train loss is 2.238518476486206\n",
      "Epoch: 1, loss: 2.23852 valid loss:  26494.41154 \n",
      "Epoch: 2, loss: 4.43988 valid loss:  26578.67015 \n",
      "Saved best model epoch: 3 val loss is: 26492.61336929764 train loss is 4.579494953155518\n",
      "Epoch: 3, loss: 4.57949 valid loss:  26492.61337 \n",
      "Saved best model epoch: 4 val loss is: 25934.172495182116 train loss is 2.0550918579101562\n",
      "Epoch: 4, loss: 2.05509 valid loss:  25934.17250 \n",
      "Epoch: 5, loss: 1.56111 valid loss:  25950.88703 \n",
      "Epoch: 6, loss: 1.19161 valid loss:  26572.51952 \n",
      "Saved best model epoch: 7 val loss is: 25916.932486341822 train loss is 1.9410923719406128\n",
      "Epoch: 7, loss: 1.94109 valid loss:  25916.93249 \n",
      "Epoch: 8, loss: 0.42165 valid loss:  26526.96840 \n",
      "Saved best model epoch: 9 val loss is: 24764.779407531023 train loss is 4.697839736938477\n",
      "Epoch: 9, loss: 4.69784 valid loss:  24764.77941 \n",
      "Epoch: 10, loss: 0.68194 valid loss:  26524.88758 \n",
      "Epoch: 11, loss: 225.65501 valid loss:  26217.46021 \n",
      "Saved best model epoch: 12 val loss is: 24714.714300878797 train loss is 3.3516149520874023\n",
      "Epoch: 12, loss: 3.35161 valid loss:  24714.71430 \n",
      "Saved best model epoch: 13 val loss is: 22099.64003875123 train loss is 38.207786560058594\n",
      "Epoch: 13, loss: 38.20779 valid loss:  22099.64004 \n",
      "Epoch: 14, loss: 147.55603 valid loss:  23451.86543 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-218-c4d3fc28ab16>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m             \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlstm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlstm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "used_seq = 3\n",
    "well_len_lst = []\n",
    "best_val_loss = 50000\n",
    "X = []\n",
    "Y = []\n",
    "loss_lst = []\n",
    "input_size = num_of_features\n",
    "lstm = LSTM2(num_classes, input_size, hidden_size, num_layers)\n",
    "lstm.to(device)\n",
    "\n",
    "lstm.apply(init_weights)\n",
    "\n",
    "criterion = torch.nn.MSELoss().to(device)    # mean-squared error for regression\n",
    "optimizer = torch.optim.Adam(lstm.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=50, factor =0.5, min_lr=1e-7, eps=1e-08)\n",
    "\n",
    "lamda = 1\n",
    "num_epochs = 100\n",
    "\n",
    "train_input_path = current_path + '\\\\Data\\\\After_Preprocess\\\\' + use_datetime + '\\\\Train\\\\'\n",
    "train_file_list = os.listdir(train_input_path)\n",
    "\n",
    "valid_input_path = current_path + '\\\\Data\\\\After_Preprocess\\\\' + use_datetime + '\\\\Valid\\\\'\n",
    "valid_file_list = os.listdir(valid_input_path)\n",
    "\n",
    "for epoch in progress_bar(range(num_epochs)):\n",
    "    np.random.shuffle(train_file_list)\n",
    "    loss_lst = []\n",
    "    val_loss_lst = []\n",
    "    lstm.train()\n",
    "    for file in train_file_list:\n",
    "        train_dataloader = ReturnDataLoader(train_input_path, file, used_seq)\n",
    "        for data in train_dataloader:\n",
    "            x_train, y_train = data\n",
    "            outputs = lstm(x_train.to(device))\n",
    "            optimizer.zero_grad()\n",
    "            torch.nn.utils.clip_grad_norm_(lstm.parameters(), 1)\n",
    "            # restriction = Cal_Restriction(outputs)\n",
    "            # loss = lamda * criterion(outputs[:, 0], y_train[:, 0].to(device)) + (1-lamda) * criterion(outputs[:, 1], y_train[:, 1].to(device)) + restriction\n",
    "            loss = lamda * criterion(outputs[:, 0], y_train[:, 0].to(device)) + (1-lamda) * criterion(outputs[:, 1], y_train[:, 1].to(device))\n",
    "            loss_lst = np.append(loss_lst, loss.cpu().item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    train_loss_tot = torch.tensor(loss).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        lstm.eval()\n",
    "        for file_ in valid_file_list:\n",
    "            valid_dataloader = ReturnDataLoader(valid_input_path, file_, used_seq)\n",
    "            for data_ in valid_dataloader:\n",
    "                x_valid, y_valid = data_\n",
    "                outputs_ = lstm(x_valid.to(device))\n",
    "                # restriction_ = Cal_Restriction(outputs_)\n",
    "                # val_loss = lamda * criterion(outputs_[:, 0], y_valid[:, 0].to(device)) + (1-lamda) * criterion(outputs_[:, 1], y_valid[:, 1].to(device)) + restriction_\n",
    "                val_loss = lamda * criterion(outputs_[:, 0], y_valid[:, 0].to(device)) + (1-lamda) * criterion(outputs_[:, 1], y_valid[:, 1].to(device))\n",
    "\n",
    "                val_loss_lst = np.append(val_loss_lst, val_loss.cpu().item())\n",
    "\n",
    "        val_loss_tot = torch.tensor(np.mean(val_loss_lst)).to(device)\n",
    "\n",
    "        if val_loss_tot.cpu().item() < best_val_loss:\n",
    "            torch.save(lstm.state_dict(), current_path + '\\\\Model\\\\Best_model_Case_' + name_of_used_features + '_' + str(seq_length) + 'seq_' + use_datetime + '_Test' + '.pt')\n",
    "            print(\"Saved best model epoch:\", epoch, \"val loss is:\", val_loss_tot.cpu().item(), \"train loss is\", train_loss_tot.cpu().item())\n",
    "            best_val_loss = val_loss_tot.cpu().item()\n",
    "\n",
    "        if epoch % 1 == 0:\n",
    "            print(\"Epoch: %d, loss: %1.5f valid loss:  %1.5f \" %(epoch, train_loss_tot.cpu().item(), val_loss_tot.cpu().item()))\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "joko",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15 (default, Nov 24 2022, 18:44:54) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "75fd4db7794c7827b47525f9788cc78c9c08a58097a2070da1f1b152337066ab"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
